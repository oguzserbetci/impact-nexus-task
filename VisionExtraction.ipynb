{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP task:\n",
    "1. Please develop a program in python that extracts the vision/mission of the company out of the example text we provide. Please share the results in a commented jupyter notebook that explains your solution (please justify your model choice). \n",
    "2. Explain the challenges that lie in the extraction of the service/product of a business for the example texts. Write down some ideas how you would approach the task.\n",
    "Attached you find\n",
    "â€¢ We shared 3 example jsons to work with (as a warning, there is a lot of noise in the data, so you have to identify the right sections!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_path in Path('data/').glob('*.json'):\n",
    "    with open(json_path) as f:\n",
    "        scrape = json.load(f)\n",
    "        scrape['json_path'] = str(json_path)\n",
    "        scrapes[scrape['website']['company_name']] = scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyperdrive Innovation', 'Black Bear Carbon', 'INFARM']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(scrapes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperdrive Innovation has 48 pages scraped\n",
      "Black Bear Carbon has 27 pages scraped\n",
      "INFARM has 38 pages scraped\n"
     ]
    }
   ],
   "source": [
    "company_name = 'Black Bear Carbon'\n",
    "for company, scrape in scrapes.items():\n",
    "    print(company, 'has', len(scrape['website']['website_content'].keys()), 'pages scraped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission / Vision extraction\n",
    "Because of the limited number of data available, best initial solution is to do manual feature engineering. \n",
    "This should be a good baseline for a supervised classification task we might implement later.\n",
    "\n",
    "Below approach is straight forward. Using extensions on spacy docs, we define mission sentence as having any predefined keywords, which are picked manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "mission_words = ('mission', 'vision', 'idea', 'goal', 'solve', 'believe')\n",
    "is_mission_getter = lambda token: token.lemma_.lower() in mission_words\n",
    "has_mission_getter = lambda obj: any([t.lemma_.lower() in mission_words for t in obj])\n",
    "\n",
    "Token.set_extension(\"is_mission_word\", getter=is_mission_getter)\n",
    "Span.set_extension(\"has_mission_word\", getter=has_mission_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Content:\n",
    "    identifier: str\n",
    "    content: str\n",
    "    source: str\n",
    "    doc: Doc = field(init=False)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.doc = nlp(self.content)\n",
    "        \n",
    "    def get_sentences(self):\n",
    "        return self.doc.sents\n",
    "    \n",
    "    def get_mission_sentences(self):\n",
    "        return list(filter(lambda x: x._.has_mission_word, self.get_sentences()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_website_content_with_regex(scrape, regex=r'(about)'):\n",
    "    contents = []\n",
    "    for page_url, content in scrape.get('website', {}).get('website_content', {}).items():\n",
    "        if re.findall(regex, page_url):\n",
    "            contents.append(Content(page_url, content, 'website'))\n",
    "    return contents\n",
    "\n",
    "def get_twitter_content(scrape):\n",
    "    contents = []\n",
    "    for dic in scrape.get('Twitter_account', []):\n",
    "        contents.append(Content(dic['id_str'], dic['text'], 'twitter'))\n",
    "    return contents\n",
    "\n",
    "def get_medium_content(scrape):\n",
    "    contents = []\n",
    "    for dic in scrape.get('medium', []):\n",
    "        contents.append(Content(dic['url'], dic['text'], 'medium'))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scrape_contents = defaultdict(list)\n",
    "for company, scrape in scrapes.items():\n",
    "    scrape_contents[company] += get_website_content_with_regex(scrape, r'https?://[^/]+/?$')\n",
    "    scrape_contents[company] += get_website_content_with_regex(scrape, r'https?://.*/[^/]*(about)[^/]*$')\n",
    "    scrape_contents[company] += get_twitter_content(scrape)\n",
    "    scrape_contents[company] += get_medium_content(scrape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company, contents in scrape_contents.items():\n",
    "    print(company)\n",
    "    all_mission_sentences = [c.get_mission_sentences() for c in contents]\n",
    "    print(sum(all_mission_sentences, []), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impact-nexus-task",
   "language": "python",
   "name": "impact-nexus-task"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
